---
phase: 02-core-agent-loop-basic-tools
plan: 03
type: execute
wave: 3
depends_on: ["02-01", "02-02"]
files_modified:
  - src/agent/mod.rs
  - src/agent/agent_loop.rs
  - src/main.rs
autonomous: true

must_haves:
  truths:
    - "The harness validates Ollama connectivity and model availability at startup before entering the agent loop"
    - "The agent loop streams model text to stdout in real time and processes tool calls through the dispatch function"
    - "When the model responds with text only (no tool call), the harness adds it to conversation and re-prompts"
    - "Ctrl+C finishes the current turn gracefully; second Ctrl+C force-kills"
    - "All events (text, tool calls, tool results, errors) are logged to the JSONL session file"
    - "The loop runs continuously until Ctrl+C or context window fills"
  artifacts:
    - path: "src/agent/agent_loop.rs"
      provides: "Core agent conversation loop with streaming, tool dispatch, and shutdown handling"
      min_lines: 150
    - path: "src/main.rs"
      provides: "Updated entry point that runs the agent loop in the Run command"
      contains: "run_agent_loop"
  key_links:
    - from: "src/agent/agent_loop.rs"
      to: "src/agent/tools.rs"
      via: "define_tools() and dispatch_tool_call()"
      pattern: "define_tools|dispatch_tool_call"
    - from: "src/agent/agent_loop.rs"
      to: "src/agent/system_prompt.rs"
      via: "build_system_prompt()"
      pattern: "build_system_prompt"
    - from: "src/agent/agent_loop.rs"
      to: "src/agent/logging.rs"
      via: "SessionLogger for JSONL event logging"
      pattern: "SessionLogger"
    - from: "src/agent/agent_loop.rs"
      to: "genai"
      via: "Client::default(), exec_chat_stream, ChatRequest, ChatStreamEvent"
      pattern: "exec_chat_stream"
    - from: "src/main.rs"
      to: "src/agent/agent_loop.rs"
      via: "run_agent_loop() call in Run command handler"
      pattern: "run_agent_loop"
---

<objective>
Implement the core agent conversation loop, Ollama health check, Ctrl+C signal handling, and wire everything into main.rs.

Purpose: This is the capstone plan -- it brings together the session logger (Plan 01), system prompt + tools (Plan 02), and the genai client into a working infinite conversation loop. After this plan, `ouro run` starts a real agent session against a local Ollama model.

Output: A working agent loop that connects to Ollama, streams model text to stdout, dispatches tool calls, logs everything to JSONL, and handles graceful shutdown via Ctrl+C.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-agent-loop-basic-tools/02-CONTEXT.md
@.planning/phases/02-core-agent-loop-basic-tools/02-RESEARCH.md
@.planning/phases/02-core-agent-loop-basic-tools/02-01-SUMMARY.md
@.planning/phases/02-core-agent-loop-basic-tools/02-02-SUMMARY.md

# All agent module files built by prior plans
@src/agent/mod.rs
@src/agent/logging.rs
@src/agent/system_prompt.rs
@src/agent/tools.rs

# Existing files to modify
@src/main.rs
@src/safety/mod.rs
@src/config/schema.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Ollama health check and agent loop</name>
  <files>
    src/agent/agent_loop.rs
    src/agent/mod.rs
  </files>
  <action>
Implement `src/agent/agent_loop.rs` with the Ollama health check and the core conversation loop.

**Ollama health check function:**
```rust
async fn check_ollama_ready(model: &str) -> anyhow::Result<()>
```
- Step 1: HTTP GET to `http://localhost:11434/` with 5-second timeout. If unreachable, return `AgentError::OllamaUnavailable` with a clear message ("Is Ollama running?").
- Step 2: HTTP POST to `http://localhost:11434/api/show` with `{"model": model}` and 10-second timeout. If model not found (non-200), return `AgentError::ModelNotAvailable` with a message suggesting `ollama pull {model}`.
- Use `reqwest::Client::new()` for HTTP calls (genai depends on reqwest, so it's already available).

**Core agent loop function:**
```rust
pub async fn run_agent_loop(
    config: &AppConfig,
    safety: &SafetyLayer,
) -> anyhow::Result<()>
```

**Loop implementation:**

1. **Startup:**
   - Call `check_ollama_ready(&config.model).await?` -- fail early if Ollama or model not ready
   - Create `SessionLogger::new(&config.workspace)?`
   - Build system prompt via `build_system_prompt(&config.workspace, &config.model, &tool_descriptions()).await?`
   - Create genai `Client::default()`
   - Build initial `ChatRequest::from_system(system_prompt).with_tools(define_tools())`
   - Set up `ChatOptions::default().with_capture_content(true).with_capture_tool_calls(true)`
   - Log `session_start` event
   - Print startup info to stderr (model, workspace, log file path)

2. **Signal handling setup:**
   - Create `Arc<AtomicBool>` shutdown flag (initially false)
   - Spawn a tokio task that waits for `tokio::signal::ctrl_c()`:
     - First Ctrl+C: set shutdown flag to true, print message to stderr ("Shutting down after current turn... Ctrl+C again to force quit")
     - Second Ctrl+C: print force-quit message, call `std::process::exit(1)`

3. **Main loop (each iteration is one "turn"):**
   - Check shutdown flag at start of each turn -- if set, break
   - Increment turn counter
   - Call `client.exec_chat_stream(&config.model, chat_req.clone(), Some(&chat_options)).await`
   - If this errors, log the error, print to stderr, and break (don't crash silently)
   - Iterate over the stream using `futures::StreamExt::next()`:
     - `ChatStreamEvent::Chunk(chunk)`: print `chunk.content` to stdout, flush stdout immediately for real-time streaming feel
     - `ChatStreamEvent::End(end)`: capture tool calls via `end.captured_into_tool_calls()`, capture text via `end.captured_first_text()`
     - Other events (Start, ReasoningChunk, etc.): ignore

4. **After streaming completes for a turn:**
   - If the model produced text content: log `assistant_text` event
   - If tool_calls is empty (text-only response -- "thinking out loud"):
     - Append the text as `ChatMessage::assistant(text)` to chat_req
     - Print a newline after the streamed text
     - Continue to next iteration (re-prompt)
   - If tool_calls is non-empty:
     - Print a newline after any streamed text
     - Use `chat_req.append_tool_use_from_stream_end(end)` to properly append the assistant tool-call message (this preserves call_id matching). If this method proves problematic, fall back to manually constructing the assistant message.
     - For each tool call:
       - Log `tool_call` event (fn_name, fn_arguments, call_id)
       - Print tool call info to stderr: `[tool] {fn_name}({args_summary})`
       - Call `dispatch_tool_call(&call, safety, &config.workspace).await`
       - Log `tool_result` event (call_id, fn_name, result)
       - Print abbreviated result to stderr: `[result] {first_200_chars}...`
       - Append `ToolResponse::new(call.call_id.clone(), result)` to chat_req
     - Continue to next iteration

5. **Context full detection (heuristic, pre-Phase 3):**
   - Track a rough character count of all messages in the conversation
   - If the character count exceeds `config.context_limit * 4` (rough chars-to-tokens ratio), log a warning and break with a message: "Context window estimated full after {turns} turns. Restart the session."
   - This is a conservative heuristic; Phase 3 will implement proper token tracking

6. **Configurable delay between turns:**
   - If `config` has a turn_delay field > 0, sleep for that duration between turns using `tokio::time::sleep`. For now, since AppConfig doesn't have this field yet, skip the delay (default 0 per context decision). The loop structure should make adding this trivial later.

7. **Loop exit:**
   - Log `session_end` event with total_turns and reason (user_shutdown, context_full, or error)
   - Print summary to stderr: "Session ended: {reason}. {turns} turns completed. Log: {log_path}"

**Also:** Add `pub mod agent_loop;` to `src/agent/mod.rs`.

**Key patterns from research to follow:**
- Clone `chat_req` when passing to `exec_chat_stream` (it needs ownership for the stream)
- After streaming, update the original `chat_req` (not the clone) with new messages
- Flush stdout after each chunk for real-time streaming
- Check shutdown flag ONLY between turns (not mid-stream) to avoid partial state
- Use `eprintln!` for harness messages (tool call info, status), `print!` for model text (stdout)
  </action>
  <verify>
Run `cargo check` -- must compile. Run `cargo test` -- all existing tests must pass. The agent loop function itself is hard to unit test (requires Ollama), but verify it compiles and all type signatures are correct. The health check can be tested to verify it returns the correct error types when Ollama is not running (if Ollama is not available in the test environment, catch the error and verify its type).
  </verify>
  <done>
`run_agent_loop` implements the full conversation loop: health check -> session logging -> streaming model output to stdout -> tool call dispatch -> re-prompt on text-only responses -> context-full detection -> Ctrl+C shutdown. `check_ollama_ready` validates connectivity and model availability at startup. All events are logged to JSONL. `cargo check` passes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire agent loop into main.rs</name>
  <files>
    src/main.rs
  </files>
  <action>
Update `src/main.rs` to call the agent loop when the `Run` command is executed.

**Changes to the `Run` arm:**
Replace the existing placeholder code (the "Waiting for agent loop (Phase 2)" print statement) with:

```rust
cli::Commands::Run { .. } => {
    let safety = SafetyLayer::new(&config)?;

    tracing::info!(
        model = %config.model,
        workspace = %safety.workspace_root().display(),
        timeout_secs = config.shell_timeout_secs,
        blocklist_patterns = config.blocked_patterns.len(),
        "Safety layer initialized"
    );

    // Run the agent loop -- this blocks until shutdown or context full
    agent::agent_loop::run_agent_loop(&config, &safety).await?;
}
```

Remove the old placeholder println! that says "Waiting for agent loop (Phase 2)".

The `use` statements in main.rs should already have `mod agent;` from Plan 01. Ensure `agent::agent_loop::run_agent_loop` is accessible.

**Important:** The function is async and returns `Result<()>`. Errors should propagate up to main's `anyhow::Result<()>` return. The `#[tokio::main]` attribute on main already provides the async runtime.
  </action>
  <verify>
Run `cargo check` -- must compile with the new main.rs wiring. Run `cargo test` -- all tests pass. Run `cargo build` to produce the binary. Optionally, run `cargo run -- run --workspace /tmp/test-workspace` to verify it attempts to connect to Ollama (it will fail with a clear error if Ollama is not running, which is the expected behavior -- the health check should produce a user-friendly error message).
  </verify>
  <done>
`ouro run` now calls `run_agent_loop` which validates Ollama, loads system prompt, and enters the conversation loop. The placeholder message is gone. Running without Ollama produces a clear "Cannot connect to Ollama" error. `cargo build` succeeds. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `cargo check` passes
2. `cargo test` passes (all tests)
3. `cargo build` produces a working binary
4. `ouro run --workspace /tmp/test-ws` with Ollama NOT running produces a clear error about Ollama connectivity
5. With Ollama running and a valid model: the agent streams text to stdout, processes tool calls, and logs to JSONL
6. Ctrl+C produces graceful shutdown message; second Ctrl+C force-exits
7. JSONL log file is created with session_start and session_end events
</verification>

<success_criteria>
The complete agent loop is functional: `ouro run` validates Ollama connectivity, loads SYSTEM_PROMPT.md from the workspace, enters an infinite conversation loop that streams model text to stdout, dispatches tool calls (shell, file read, file write), logs all events to JSONL, and shuts down gracefully on Ctrl+C or when context is estimated full. The harness connects to a local Ollama model and sustains multi-turn conversation without manual intervention.
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-agent-loop-basic-tools/02-03-SUMMARY.md`
</output>
